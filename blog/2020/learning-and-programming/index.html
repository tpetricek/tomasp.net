<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8">
  <meta name=viewport content="width=device-width, initial-scale=1">
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Droid+Sans+Mono&family=Kreon:wght@300;400;500;600&family=PT+Sans:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <link rel="alternate" type="application/rss+xml" title="Latest news from Tomas Petricek" href="/rss.xml" />
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <link href="/custom/style.css" rel="stylesheet">
  <link href="/custom/tooltips.css" rel="stylesheet">
  <script src="/custom/tooltips.js" type="text/javascript"></script>
  <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon-big.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/img/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/img/favicon-big.png">
  <meta name="msapplication-TileColor" content="#004C6B">
  <meta name="msapplication-TileImage" content="/img/favicon-big.png">
  <meta name="theme-color" content="#004C6B">
  
  <title>Is deep learning a new kind of programming? Operationalistic look at programming - Tomas Petricek</title>

  <meta name="description" content=" Making a concept synonymous with the operations for working with it is
an idea introduced by P. W. Bridgman to think about measurements in physics. A length
measured by a ruler is a different concept than a length measured by the time it takes
light to travel. However, what if we thought about programs in the same way? Is a program
constructed manually the same as a program obtained by training a neural network?" />
  <meta name="keywords" content="programming languages, philosophy, research, " />  
  <meta name="author" content="Tomas Petricek" />
  <meta name="copyright" content="Tomas Petricek" />
  
  <meta property="og:title" content="Is deep learning a new kind of programming? Operationalistic look at programming" />
  <meta property="og:type" content="article" />
  <meta property="og:url" content="http://tomasp.net/blog/2020/learning-and-programming/" />
  <meta property="og:image" content="http://tomasp.net/blog/2020/learning-and-programming/pyrometer.jpg" />
  <meta property="og:description" content=" Making a concept synonymous with the operations for working with it is
an idea introduced by P. W. Bridgman to think about measurements in physics. A length
measured by a ruler is a different concept than a length measured by the time it takes
light to travel. However, what if we thought about programs in the same way? Is a program
constructed manually the same as a program obtained by training a neural network?" />
  
  <meta name="twitter:card" content="summary_large_image">
  
  <meta name="twitter:site" content="@tomaspetricek">
  <meta name="twitter:creator" content="@tomaspetricek">  
  <meta name="twitter:title" content="Is deep learning a new kind of programming? Operationalistic look at programming" />
  <meta name="twitter:image" content="http://tomasp.net/blog/2020/learning-and-programming/pyrometer.jpg" />
  <meta name="twitter:description" content=" Making a concept synonymous with the operations for working with it is
an idea introduced by P. W. Bridgman to think about measurements in physics. A length
measured by a ruler is a different concept than a length measured by the time it takes
light to travel. However, what if we thought about programs in the same way? Is a program
constructed manually the same as a program obtained by training a neural network?" />
    
  <script type="application/ld+json">
  {
  	"@context": "http:\/\/schema.org",
  	"@type": "Article",
  	"name": "Is deep learning a new kind of programming? Operationalistic look at programming",
    "headline": "Is deep learning a new kind of programming? Operationalistic look at programming",
  	"description": " Making a concept synonymous with the operations for working with it is
an idea introduced by P. W. Bridgman to think about measurements in physics. A length
measured by a ruler is a different concept than a length measured by the time it takes
light to travel. However, what if we thought about programs in the same way? Is a program
constructed manually the same as a program obtained by training a neural network?",
  	"url": "http://tomasp.net/blog/2020/learning-and-programming/",
  	"author": {
  		"@type": "Person",
  		"name": "Tomas Petricek",
  		"url": "http://tomasp.net",
  		"sameAs": ["http://twitter.com/tomaspetricek"]
  	},
  	"creator": ["Tomas Petricek"],
  	"dateCreated": "2020-10-07T02:43:16.3109375+02:00",
  	"datePublished": "2020-10-07T02:43:16.3109375+02:00",
    "dateModified": "2020-10-07T02:43:16.3109375+02:00",
    "mainEntityOfPage": "http://tomasp.net/blog/2020/learning-and-programming/",
  	"image": "http://tomasp.net/blog/2020/learning-and-programming/pyrometer.jpg",
  	"thumbnailUrl": "http://tomasp.net/blog/2020/learning-and-programming/pyrometer.jpg",
  	"keywords": ["programming languages", "philosophy", "research",  "tomas", "petricek"],
  	"inLanguage": "en-us",
  	"publisher": {
  		"@type": "Person",
  		"name": "Tomas Petricek",
  		"url": "http://tomasp.net",
      "logo": "http://tomasp.net/img/favicon-big.png",
  		"sameAs": ["http://twitter.com/tomaspetricek"]
  	}
  }  
  </script>

</head>
<body class="default">
    
  <span class="tplink"><a href="/">TP</a></span>

  <article class='article''>
    <h1><span class="hmq">Is deep learning a new kind of programming</span><span class="hs"> Operationalistic look at programming</span></h1>
<p>In most discussions about how to make programming better, someone eventually says
something along the lines of <em>"we'll just have to wait until deep learning solves
the problem!"</em> I think this is a <a href="https://en.wikipedia.org/wiki/AI_winter">naively optimistic idea</a>,
but it raises one interesting question: In what sense are programs created using deep
learning a <em>different kind</em> of programs than those written by hand?</p>
<div class="rdecor">
<img src="http://tomasp.net/blog/2020/learning-and-programming/bridgman.jpg" style="max-width:350px"/>
</div>
<p>This question recently arose in discussions that we have been having as part of the
<a href="https://programme.hypotheses.org/">PROGRAMme project</a>, which explores historical and
philosophical perspectives on the question "What is a (computer) program?" and so this
article owes much debt to <a href="https://programme.hypotheses.org/members">others involved in the project</a>,
especially Maël Pégny, Liesbeth De Mol and Nick Wiggershaus.</p>
<p>Many people will intuitively think that, if you train a deep neural network to solve some
a problem, you get a different kind of program than if you manually write some logic to solve
the problem. But what exactly is the difference? In both cases, the program is a sequence of
instructions that are deterministically executed by a machine, one after another, to produce
the result.</p>
<p>When reading the excellent book <a href="https://amzn.to/2SvTwKT" title="Hasok Chang (2004). Inventing Temperature: Measurement and Scientific Progress">Inventing Temperature</a> by Hasok
Chang recently, I came across the idea of <a href="https://plato.stanford.edu/entries/operationalism/" title="Hasok Chang (2019). Operationalism, The Stanford Encyclopedia of Philosophy">operationalism</a>,
which I believe provides a useful perspective for thinking about the issue of deep learning and
programming. The operationalist point of view was introduced by a physicist Percy Williams Bridgman. To
quote: <em>we mean by any concept nothing more than a set of operations; the concept is synonymous
with the corresponding set of operations</em>. What does this tell us about deep learning and programming?</p>
<h2>Operationalism and temperature</h2>
<p>Before I talk about programming, I need to say a bit about <em>operationalism</em>. I will be relying on
the description from <a href="https://amzn.to/2SvTwKT">Chang's book on Temperature</a>, but Bridgman used
the measurement of length as an example. The key idea is that a concept, such as a length, is
defined by the operations for working with it. This means that the <em>length</em> that is measured
using a ruler is a different kind of <em>length</em> than an astronomical length measured in terms of
the amount of time that light takes to travel.</p>
<div class="rdecor">
<img src="http://tomasp.net/blog/2020/learning-and-programming/pyrometer.jpg" style="max-width:350px"/>
</div>
<p>We are so used to thinking of <em>length</em> that this idea may seem odd, but take temperature
measurement as an example. One issue in the history of temperature was that regular thermometers
did not work for very high temperatures (the boiling point of mercury is 356.7 °C). An
alternative way of measuring very high temperatures was invented by <a href="https://en.wikipedia.org/wiki/Wedgwood_scale">Josiah Wedgwood</a>,
an English potter. His Wedgwood scale (°W) was based on the shrinking of small clay cylinders.
In high heat, the cylinders contracted. After removing them from the heat, you then used a
provided ruler to read the temperature. (This only worked with a specific clay from Wedgwood's
own mines, but he kindly donated all the clay to the Royal Society.)
The temperature of <a href="https://en.wikipedia.org/wiki/Red_heat">red heat</a> was 0 °W, melting point
of copper 27 °W and the melting point of gold was 32 °W.</p>
<p>Figuring out how to match a temperature scale based on mercury thermometer with the Wedgwood
scale is hard, because the two do not operationally overlap. Wedgwood's own attempt at producing
a conversion table was a way off, giving melting point of silver as 4,717 °F (rather than 1,763°F).</p>
<h2>Operationalist look at programming and machine learning</h2>
<p>Programming is not much like measuring temperature, but there are certainly practical "operations"
that are employed for doing various things with programs. Many of the operations that you use
when creating a program based on deep learning and an ordinary program are quite different.</p>
<ul>
<li>
<p><strong>Program execution.</strong>
First of all, execution is the operation that is actually very similar for both regular programs
and deep neural networks. In both cases, the program is a long sequence of instructions
with some data. It is provided with some inputs, performs a calculation using the inputs and
produces an output. If we looked only at execution, then we would likely not see much difference.</p>
</li>
<li>
<p><strong>Programming or training.</strong>
The difference becomes obvious when we start looking at how programs are constructed. In case of
ordinary programs, you write some logic. In case of deep networks (or any other programs based on
machine learning), the program is obtained by training, i.e. adapting some numerical parameters
based on sample data.</p>
</li>
<li>
<p><strong>Understanding programs.</strong>
A more interesting issue is that of understanding what a program does. This may be a challenge
for an ordinary program if it is large and complicated, but you can generally study the code
or ask people who wrote it. For machine learning, this depends on the type of algorithm used.
While you can understand how a decision tree makes a decision, it is not clear how to "understand"
what a deep neural network does.</p>
</li>
<li>
<p><strong>Verifying programs.</strong>
If you wanted to prove that an ordinary program is correct, you can (perhaps very laboriously)
prove that it matches its specification, which describes its key properties. For deep neural
network, you can verify that it correctly propagates weights, but proving anything about what
the program actually does is tricky. (Incidentally, this is also why I always found that the 2015
<a href="https://futureoflife.org/ai-open-letter/" title="Stuart Russell, Daniel Dewey, Max Tegmark (2015). An Open Letter: Research Priorities for Robust and Beneficial Artificial Intelligence">AI Open Letter</a> is missing the point in its emphasis
of "Verification" as a research goal for AI systems.)</p>
</li>
</ul>
<h2>Opacity of programs and ML algorithms</h2>
<div class="rdecor">
<img src="http://tomasp.net/blog/2020/learning-and-programming/atlas.jpg" style="max-width:350px"/>
</div>
<p>In the above list, the cases of execution and programming or training show fairly obvious
similarities and differences, respectively. The more subtle cases are that of understanding
and verifying programs. An interesting reference for thinking about these is the paper
<a href="http://philsci-archive.pitt.edu/17637/" title="Florian J. Boge, Paul Grünke (2019). Computer simulations, machine learning and the Laplacean demon: Opacity in the case of high energy physics">Computer simulations, machine learning and the Laplacean demon: Opacity in the case of high energy
physics</a>, which looks at programs and ML algorithms in
physics experiments at CERN (thanks to Nick for the recommendation!)</p>
<p>The paper compares opacity, i.e. the possibility of understanding, of deep neural networks and
computer simulations, which are very large and complex, but otherwise "ordinary" programs.
It identifies a number of different kinds of opacity.</p>
<p>Both computer simulations and deep networks are <em>algorithmically transparent</em>, which means that
one can follow the sequence of instructions when they execute. To a human, this does not help
very much when trying to understand a program (but it is enough for a <a href="https://en.wikipedia.org/wiki/Laplace%27s_demon">Laplacean deamon</a>,
and so neither of the types of programs have what the authors call <em>fundamental opacity</em>).</p>
<h4>Complexity of large systems</h4>
<p>The opacity is then closely linked to the humans involved in the process.
For computer simulations, the main issue is that they are large and complex.
This is, by the way, the case for many other software systems and we could
reasonably argue that large systems (or simulations) are different kind of
programs than small ones, precisely because the operation of "understanding them"
is different for each. To quote Bridgman again:</p>
<blockquote>
<p><em>Mathematics does not recognize that as the physical range increases, the fundamental
concepts become hazy, and eventually cease entirely to have physical meaning
and therefore must be replaced by other concepts which are operationally quite different.</em></p>
</blockquote>
<p>I think the issue of scale is something that computer scientists ignore way
too easily. For example, the methods that we use for proving the correctness of
a small, several line long, algorithm are operationally very different than those
we use for proving the correctness of a <a href="https://cakeml.org/">compiler for a realistic programming language</a>.</p>
<h4>The case of deep learning algorithms</h4>
<p>The issue of complexity is even worse in the case of deep neural networks.
A useful concept referenced in the above paper is "opaqueness of paths".
In a deep neural network (with non-zero weight), the number of possible paths
through which data can flow increases exponentially with every layer. The number
of data-dependencies in a complex ordinary program may be large too, but it
won't grow exponentially with e.g. every new class.</p>
<p>The above paper makes one more excellent point about machine learning. The
ML algorithm has a <em>quasi-autonomy</em> and performs iterative tuning of parameters.
For ordinary programs, the programmer writes every single line (and tunes all constants)
of the program. In other words, even if we cannot immediately understand every
single aspect of an ordinary program, we should be able to find someone who knows
(or at least knew at some point in the past). For understanding a weight in a
deep network, we'd need to backtrack through the entire learning process...</p>
<h2>Towards a unified understanding?</h2>
<p>Looking at programs and machine learning from an operationalist perspective also
suggests an interesting future research question. If we have two measurement operations
that can both be applied over some domain, it is possible to make the operations
match over the common domain and unify the scales. Wedgewood himself tried to do this
by matching his scale to the Fahrenheit scale by using a scale based on metal expansion
as the intermediary between mercury based thermometers and his clay cylinders.
(The Wedgwood scale was forgotten before anybody produced a <em>correct mapping</em>, but
a correct mapping could certainly be created.)</p>
<p>The curious question I want to conclude with is this: Are there operations for
construction, understanding and verification that would work the same for both
"ordinary" programs and machine learning algorithms? Certainly not today and certainly not
anytime soon. However, I imagine that there might be a more interactive, machine-assisted way of
programming and doing some of the other tasks that can work with both kinds of entities -
ordinary programs at one end of the spectrum and deep neural networks at the other end.</p>
<h2>References</h2>
<ol>
<li><a href='https://amzn.to/2SvTwKT'>Hasok Chang (2004). Inventing Temperature: Measurement and Scientific Progress</a></li>
<li><a href='https://plato.stanford.edu/entries/operationalism/'>Hasok Chang (2019). Operationalism, The Stanford Encyclopedia of Philosophy</a></li>
<li><a href='https://futureoflife.org/ai-open-letter/'>Stuart Russell, Daniel Dewey, Max Tegmark (2015). An Open Letter: Research Priorities for Robust and Beneficial Artificial Intelligence</a></li>
<li><a href='http://philsci-archive.pitt.edu/17637/'>Florian J. Boge, Paul Grünke (2019). Computer simulations, machine learning and the Laplacean demon: Opacity in the case of high energy physics</a></li>
</ol>




    <div class="info">
    <p class="share">
        <a target="_blank" href="https://twitter.com/intent/tweet?url=http%3a%2f%2ftomasp.net%2fblog%2f2020%2flearning-and-programming%2f&amp;text=Is+deep+learning+a+new+kind+of+programming%3f+Operationalistic+look+at+programmingvia+%40tomaspetricek">
          <i class="fab fa-twitter-square"></i></a>
        <a target="_blank" href="https://www.facebook.com/sharer.php?u=http%3a%2f%2ftomasp.net%2fblog%2f2020%2flearning-and-programming%2f">
          <i class="fab fa-facebook-square"></i></a>
        <a href="http://www.reddit.com/submit?url=http%3a%2f%2ftomasp.net%2fblog%2f2020%2flearning-and-programming%2f&title=Is+deep+learning+a+new+kind+of+programming%3f+Operationalistic+look+at+programming">
          <i class="fab fa-reddit-square"></i></a>
        <a href="mailto:?subject=Is%20deep%20learning%20a%20new%20kind%20of%20programming%3f%20Operationalistic%20look%20at%20programming&body=%20Making%20a%20concept%20synonymous%20with%20the%20operations%20for%20working%20with%20it%20is%0d%0aan%20idea%20introduced%20by%20P.%20W.%20Bridgman%20to%20think%20about%20measurements%20in%20physics.%20A%20length%0d%0ameasured%20by%20a%20ruler%20is%20a%20different%20concept%20than%20a%20length%20measured%20by%20the%20time%20it%20takes%0d%0alight%20to%20travel.%20However%2c%20what%20if%20we%20thought%20about%20programs%20in%20the%20same%20way%3f%20Is%20a%20program%0d%0aconstructed%20manually%20the%20same%20as%20a%20program%20obtained%20by%20training%20a%20neural%20network%3f%0a%0aSee%3a%20http%3a%2f%2ftomasp.net%2fblog%2f2020%2flearning-and-programming%2f">
          <i class="fa fa-envelope"></i></a>
    </p>
    <p class="details">
      <strong>Published:</strong> Wednesday, 7 October 2020, 2:43 AM<br />
      <strong>Author:</strong> Tomas Petricek<br />
      <strong>Typos:</strong> <a href="http://github.com/tpetricek/tomasp.net">Send me a pull request</a>!<br />
      
        <strong>Tags:</strong> <a
          href="/blog/tag/programming-languages/">programming languages</a>, <a
          href="/blog/tag/philosophy/">philosophy</a>, <a
          href="/blog/tag/research/">research</a></span><br />
      
    </p>
    </div>
  </article>


  <footer>
  <div class="footer-body">
    <div class="fl70"><div class="fmr">
      <h4><a href="http://tomasp.net/rss.xml"><i class="fa fa-rss" style="margin-right:5px;"></i></a> Blog archives</h4>
      <p>
      
      <a href="/blog/archive/february_2025/">February 2025 (1)</a>,&nbsp;
      
      <a href="/blog/archive/december_2023/">December 2023 (1)</a>,&nbsp;
      
      <a href="/blog/archive/february_2023/">February 2023 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2022/">September 2022 (1)</a>,&nbsp;
      
      <a href="/blog/archive/april_2022/">April 2022 (1)</a>,&nbsp;
      
      <a href="/blog/archive/october_2021/">October 2021 (1)</a>,&nbsp;
      
      <a href="/blog/archive/april_2021/">April 2021 (1)</a>,&nbsp;
      
      <a href="/blog/archive/october_2020/">October 2020 (1)</a>,&nbsp;
      
      <a href="/blog/archive/july_2020/">July 2020 (1)</a>,&nbsp;
      
      <a href="/blog/archive/april_2020/">April 2020 (2)</a>,&nbsp;
      
      <a href="/blog/archive/december_2019/">December 2019 (1)</a>,&nbsp;
      
      <a href="/blog/archive/february_2019/">February 2019 (1)</a>,&nbsp;
      
      <a href="/blog/archive/november_2018/">November 2018 (1)</a>,&nbsp;
      
      <a href="/blog/archive/october_2018/">October 2018 (1)</a>,&nbsp;
      
      <a href="/blog/archive/may_2018/">May 2018 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2017/">September 2017 (1)</a>,&nbsp;
      
      <a href="/blog/archive/june_2017/">June 2017 (1)</a>,&nbsp;
      
      <a href="/blog/archive/april_2017/">April 2017 (1)</a>,&nbsp;
      
      <a href="/blog/archive/march_2017/">March 2017 (2)</a>,&nbsp;
      
      <a href="/blog/archive/january_2017/">January 2017 (1)</a>,&nbsp;
      
      <a href="/blog/archive/october_2016/">October 2016 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2016/">September 2016 (2)</a>,&nbsp;
      
      <a href="/blog/archive/august_2016/">August 2016 (1)</a>,&nbsp;
      
      <a href="/blog/archive/july_2016/">July 2016 (1)</a>,&nbsp;
      
      <a href="/blog/archive/may_2016/">May 2016 (2)</a>,&nbsp;
      
      <a href="/blog/archive/april_2016/">April 2016 (1)</a>,&nbsp;
      
      <a href="/blog/archive/december_2015/">December 2015 (2)</a>,&nbsp;
      
      <a href="/blog/archive/november_2015/">November 2015 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2015/">September 2015 (3)</a>,&nbsp;
      
      <a href="/blog/archive/july_2015/">July 2015 (1)</a>,&nbsp;
      
      <a href="/blog/archive/june_2015/">June 2015 (1)</a>,&nbsp;
      
      <a href="/blog/archive/may_2015/">May 2015 (2)</a>,&nbsp;
      
      <a href="/blog/archive/april_2015/">April 2015 (3)</a>,&nbsp;
      
      <a href="/blog/archive/march_2015/">March 2015 (2)</a>,&nbsp;
      
      <a href="/blog/archive/february_2015/">February 2015 (1)</a>,&nbsp;
      
      <a href="/blog/archive/january_2015/">January 2015 (2)</a>,&nbsp;
      
      <a href="/blog/archive/december_2014/">December 2014 (1)</a>,&nbsp;
      
      <a href="/blog/archive/may_2014/">May 2014 (3)</a>,&nbsp;
      
      <a href="/blog/archive/april_2014/">April 2014 (2)</a>,&nbsp;
      
      <a href="/blog/archive/march_2014/">March 2014 (1)</a>,&nbsp;
      
      <a href="/blog/archive/january_2014/">January 2014 (2)</a>,&nbsp;
      
      <a href="/blog/archive/december_2013/">December 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/november_2013/">November 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/october_2013/">October 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2013/">September 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/august_2013/">August 2013 (2)</a>,&nbsp;
      
      <a href="/blog/archive/may_2013/">May 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/april_2013/">April 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/march_2013/">March 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/february_2013/">February 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/january_2013/">January 2013 (1)</a>,&nbsp;
      
      <a href="/blog/archive/december_2012/">December 2012 (2)</a>,&nbsp;
      
      <a href="/blog/archive/october_2012/">October 2012 (1)</a>,&nbsp;
      
      <a href="/blog/archive/august_2012/">August 2012 (3)</a>,&nbsp;
      
      <a href="/blog/archive/june_2012/">June 2012 (2)</a>,&nbsp;
      
      <a href="/blog/archive/april_2012/">April 2012 (1)</a>,&nbsp;
      
      <a href="/blog/archive/march_2012/">March 2012 (4)</a>,&nbsp;
      
      <a href="/blog/archive/february_2012/">February 2012 (5)</a>,&nbsp;
      
      <a href="/blog/archive/january_2012/">January 2012 (2)</a>,&nbsp;
      
      <a href="/blog/archive/november_2011/">November 2011 (5)</a>,&nbsp;
      
      <a href="/blog/archive/august_2011/">August 2011 (3)</a>,&nbsp;
      
      <a href="/blog/archive/july_2011/">July 2011 (2)</a>,&nbsp;
      
      <a href="/blog/archive/june_2011/">June 2011 (2)</a>,&nbsp;
      
      <a href="/blog/archive/may_2011/">May 2011 (2)</a>,&nbsp;
      
      <a href="/blog/archive/march_2011/">March 2011 (4)</a>,&nbsp;
      
      <a href="/blog/archive/december_2010/">December 2010 (1)</a>,&nbsp;
      
      <a href="/blog/archive/november_2010/">November 2010 (6)</a>,&nbsp;
      
      <a href="/blog/archive/october_2010/">October 2010 (6)</a>,&nbsp;
      
      <a href="/blog/archive/september_2010/">September 2010 (4)</a>,&nbsp;
      
      <a href="/blog/archive/july_2010/">July 2010 (3)</a>,&nbsp;
      
      <a href="/blog/archive/june_2010/">June 2010 (2)</a>,&nbsp;
      
      <a href="/blog/archive/may_2010/">May 2010 (1)</a>,&nbsp;
      
      <a href="/blog/archive/february_2010/">February 2010 (2)</a>,&nbsp;
      
      <a href="/blog/archive/january_2010/">January 2010 (3)</a>,&nbsp;
      
      <a href="/blog/archive/december_2009/">December 2009 (3)</a>,&nbsp;
      
      <a href="/blog/archive/july_2009/">July 2009 (1)</a>,&nbsp;
      
      <a href="/blog/archive/june_2009/">June 2009 (3)</a>,&nbsp;
      
      <a href="/blog/archive/may_2009/">May 2009 (2)</a>,&nbsp;
      
      <a href="/blog/archive/april_2009/">April 2009 (1)</a>,&nbsp;
      
      <a href="/blog/archive/march_2009/">March 2009 (2)</a>,&nbsp;
      
      <a href="/blog/archive/february_2009/">February 2009 (1)</a>,&nbsp;
      
      <a href="/blog/archive/december_2008/">December 2008 (1)</a>,&nbsp;
      
      <a href="/blog/archive/november_2008/">November 2008 (5)</a>,&nbsp;
      
      <a href="/blog/archive/october_2008/">October 2008 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2008/">September 2008 (1)</a>,&nbsp;
      
      <a href="/blog/archive/june_2008/">June 2008 (1)</a>,&nbsp;
      
      <a href="/blog/archive/march_2008/">March 2008 (3)</a>,&nbsp;
      
      <a href="/blog/archive/february_2008/">February 2008 (1)</a>,&nbsp;
      
      <a href="/blog/archive/december_2007/">December 2007 (2)</a>,&nbsp;
      
      <a href="/blog/archive/november_2007/">November 2007 (6)</a>,&nbsp;
      
      <a href="/blog/archive/october_2007/">October 2007 (1)</a>,&nbsp;
      
      <a href="/blog/archive/september_2007/">September 2007 (1)</a>,&nbsp;
      
      <a href="/blog/archive/august_2007/">August 2007 (1)</a>,&nbsp;
      
      <a href="/blog/archive/july_2007/">July 2007 (2)</a>,&nbsp;
      
      <a href="/blog/archive/april_2007/">April 2007 (2)</a>,&nbsp;
      
      <a href="/blog/archive/march_2007/">March 2007 (2)</a>,&nbsp;
      
      <a href="/blog/archive/february_2007/">February 2007 (3)</a>,&nbsp;
      
      <a href="/blog/archive/january_2007/">January 2007 (2)</a>,&nbsp;
      
      <a href="/blog/archive/november_2006/">November 2006 (1)</a>,&nbsp;
      
      <a href="/blog/archive/october_2006/">October 2006 (3)</a>,&nbsp;
      
      <a href="/blog/archive/august_2006/">August 2006 (2)</a>,&nbsp;
      
      <a href="/blog/archive/july_2006/">July 2006 (1)</a>,&nbsp;
      
      <a href="/blog/archive/june_2006/">June 2006 (3)</a>,&nbsp;
      
      <a href="/blog/archive/may_2006/">May 2006 (2)</a>,&nbsp;
      
      <a href="/blog/archive/april_2006/">April 2006 (2)</a>,&nbsp;
      
      <a href="/blog/archive/december_2005/">December 2005 (1)</a>,&nbsp;
      
      <a href="/blog/archive/july_2005/">July 2005 (4)</a>,&nbsp;
      
      <a href="/blog/archive/june_2005/">June 2005 (5)</a>,&nbsp;
      
      <a href="/blog/archive/may_2005/">May 2005 (1)</a>,&nbsp;
      
      <a href="/blog/archive/april_2005/">April 2005 (3)</a>,&nbsp;
      
      <a href="/blog/archive/march_2005/">March 2005 (3)</a>,&nbsp;
      
      <a href="/blog/archive/january_2005/">January 2005 (1)</a>,&nbsp;
      
      <a href="/blog/archive/december_2004/">December 2004 (3)</a>,&nbsp;
      
      <a href="/blog/archive/november_2004/">November 2004 (2)</a>,&nbsp;
      
      </p>
    </div></div>
    <div class="fr30"><div class="fmr">
      <h4>License and about</h4>
      <p>All articles on this site are licensed under <a href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution Share Alike</a>.
        All source code samples are licensed under the MIT License.
      </p>
      <p>This site is hosted on GitHub and is generated using <a href="https://github.com/tpetricek/FSharp.Formatting">F# Formatting</a>
        and <a href="http://dotliquidmarkup.org/">DotLiquid</a>.
        For more info, see the <a href="https://github.com/tpetricek/tomasp.net">website source on GitHub</a>.
      </p>
      <p>Please submit issues &amp; corrections on GitHub. Use pull requests for minor corrections only.</p>
      <ul>
        <li><i class="fab fa-twitter"></i> Twitter: <a href="http://twitter.com/tomaspetricek">@tomaspetricek</a></li>
        <li><i class="fab fa-github"></i> GitHub: <a href="https://github.com/tpetricek">@tpetricek</a></li>
        <li><i class="fa fa-envelope"></i> Email: <a href="mailto:tomas@tomasp.net">tomas@tomasp.net</a></li>
      </ul>
      <p style="text-align:center;margin-top:15px;"><img src="https://tomasp.net/img/cc-sa.png" alt="CC License logo" /></p>
    </div></div>
  </div>
  </footer>

  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-1561220-1']);
    _gaq.push(['_trackPageview']);
    (function () {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
</body>
</html>
